{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkm97byNDOWA"
      },
      "source": [
        "# Описание датасета, EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlNDLyq9Q010"
      },
      "source": [
        "Датасет содержит информацию о клиентах банка, которые либо ушли, либо остались клиентами банка\n",
        "\n",
        "Список фичей:\n",
        "\n",
        "    •   Customer ID: Уникальный идентификатор каждого клиента.\n",
        "    •   Surname: Фамилия клиента.\n",
        "    •   Credit Score: Числовое значение, представляющее кредитный рейтинг клиента.\n",
        "    •   Geography: Страна проживания клиента (Франция, Испания или Германия).\n",
        "    •   Gender: Пол клиента (Мужской или Женский).\n",
        "    •   Age: Возраст клиента.\n",
        "    •   Tenure: Количество лет, которое клиент обслуживается в банке.\n",
        "    •   Balance: Баланс на счёте клиента.\n",
        "    •   NumOfProducts: Количество банковских продуктов, которыми пользуется клиент (например, сберегательный счёт, кредитная карта).\n",
        "    •   HasCrCard: Наличие кредитной карты у клиента (1 = да, 0 = нет).\n",
        "    •   IsActiveMember: Является ли клиент активным членом банка (1 = да, 0 = нет).\n",
        "    •   EstimatedSalary: Предполагаемая заработная плата клиента.\n",
        "    •   Exited: Ушёл ли клиент (1 = да, 0 = нет), таргет.\n",
        "\n",
        "Задача - бинарная классификация, предсказывание вероятности ухода клиента для тестового набора данных\n",
        "\n",
        "В качестве baseline построен sample_submission.csv, в котором вероятность ухода каждого клиента - 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9VTo4pmgpem"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EeXPTP_lT5i"
      },
      "outputs": [],
      "source": [
        "!pip install ydata-profiling --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcrR14Ntsixz"
      },
      "outputs": [],
      "source": [
        "!gdown 1XCFRgfoG0mK08v1tDCgJXBcPK6PbaVQ7 # test\n",
        "!gdown 1ItKuMwuaqWcHCCmJ7Wj8neNgs1PPrpLy # train\n",
        "!gdown 1lX2th7npV67Qzd1NLgv3rvEJyHuu4ZBD # test_submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYEPQ0yMkuT3"
      },
      "source": [
        "### Предобработка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12lUtW2VgsTO"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "target = \"Exited\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mTh1xrCk0Wf"
      },
      "outputs": [],
      "source": [
        "def encode_categorical_features(train, test=None, categorical_features=['Geography', 'Gender']):\n",
        "    \"\"\"\n",
        "    Кодирует категориальные признаки с помощью LabelEncoder\n",
        "    \"\"\"\n",
        "    label_encoders = {}\n",
        "\n",
        "    for feature in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        # Обработка train\n",
        "        train[feature] = le.fit_transform(train[feature])\n",
        "\n",
        "        # Обработка test\n",
        "        if test is not None and feature in test.columns:\n",
        "            test[feature] = le.transform(test[feature])\n",
        "\n",
        "        label_encoders[feature] = le\n",
        "\n",
        "    if test is not None:\n",
        "        return train, test, label_encoders\n",
        "    else:\n",
        "        return train, label_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YO3tR7qwlWLL"
      },
      "outputs": [],
      "source": [
        "train, _ = encode_categorical_features(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw9LkhPHHLt7"
      },
      "outputs": [],
      "source": [
        "to_drop = ['id', 'CustomerId', 'Surname']\n",
        "train = train.drop(columns=to_drop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I65lY5PRF64w"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK3rsCc4lVsw"
      },
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "profile = ProfileReport(train, title=\"Final_Project\")\n",
        "\n",
        "profile.to_notebook_iframe()\n",
        "profile.to_file(\"eda_report.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2EkD4z7FPLP"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEmKfv0RD7l1"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_kkKAW7D-0j"
      },
      "outputs": [],
      "source": [
        "num_cols = train.select_dtypes(include=['number']).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD2J74oenGIC"
      },
      "outputs": [],
      "source": [
        "# График дисбаланса классов целевой переменной\n",
        "target_cnt = train[target].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=target_cnt.index, y=target_cnt.values)\n",
        "plt.title('Дисбаланс классов целевой переменной')\n",
        "plt.xlabel('Exited')\n",
        "plt.ylabel('Количество клиентов')\n",
        "plt.xticks([0, 1], ['Остался', 'Ушел'])\n",
        "\n",
        "for i, v in enumerate(target_cnt.values):\n",
        "    plt.text(i, v + 50, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeRSFCguFYPy"
      },
      "outputs": [],
      "source": [
        "# Распределение фичей\n",
        "num_cols_no_target = [col for col in num_cols if col != 'Exited']\n",
        "\n",
        "train[num_cols_no_target].hist(bins=25, figsize=(15,10))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUkcPNngFw-A"
      },
      "outputs": [],
      "source": [
        "nan_counts = train.isna().sum()\n",
        "nan_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjr-HITYHhQr"
      },
      "outputs": [],
      "source": [
        "# Расчет корреляции с целевой переменной\n",
        "target = 'Exited'\n",
        "corr = train.corr(method='pearson')\n",
        "corr_target = (\n",
        "    corr[target]\n",
        "    .drop(target)\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(6, 8))\n",
        "sns.barplot(\n",
        "    x=corr_target.values,\n",
        "    y=corr_target.index,\n",
        "    orient=\"h\"\n",
        ")\n",
        "plt.axvline(0, color=\"k\", linewidth=0.8)\n",
        "plt.title(\"Target correlation\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCPHL_i5DWIL"
      },
      "source": [
        "# Работа с аномалиями"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq0v-jzg_4M1"
      },
      "outputs": [],
      "source": [
        "# Вспомогательные импорты\n",
        "%matplotlib inline\n",
        "from IPython.display import Image, display\n",
        "from scipy.stats import zscore, t\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
        "\n",
        "def save_and_display(fig, path):\n",
        "    fig.savefig(path, bbox_inches=\"tight\", dpi=120)\n",
        "    plt.close(fig)\n",
        "    display(Image(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EigR8sj8_4M1"
      },
      "outputs": [],
      "source": [
        "# Классическая обработка выбросов\n",
        "num_all = train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "num_all = [c for c in num_all if c != target]\n",
        "binary_cols = [c for c in num_all if train[c].nunique() <= 2]\n",
        "numeric_for_outliers = [c for c in num_all if c not in binary_cols]\n",
        "\n",
        "for df in (train, test):\n",
        "    df[numeric_for_outliers] = df[numeric_for_outliers].apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "train_mean = train[numeric_for_outliers].mean()\n",
        "train_std = train[numeric_for_outliers].std(ddof=1)\n",
        "\n",
        "z_mask_train = pd.DataFrame(\n",
        "    np.abs((train[numeric_for_outliers] - train_mean) / train_std) > 3,\n",
        "    columns=numeric_for_outliers,\n",
        ")\n",
        "z_mask_test = pd.DataFrame(\n",
        "    np.abs((test[numeric_for_outliers] - train_mean) / train_std) > 3,\n",
        "    columns=numeric_for_outliers,\n",
        ")\n",
        "\n",
        "q1 = train[numeric_for_outliers].quantile(0.25)\n",
        "q3 = train[numeric_for_outliers].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "iqr_mask_train = pd.DataFrame(\n",
        "    (train[numeric_for_outliers] < lower) | (train[numeric_for_outliers] > upper),\n",
        "    columns=numeric_for_outliers,\n",
        ")\n",
        "iqr_mask_test = pd.DataFrame(\n",
        "    (test[numeric_for_outliers] < lower) | (test[numeric_for_outliers] > upper),\n",
        "    columns=numeric_for_outliers,\n",
        ")\n",
        "\n",
        "def grubbs_threshold(series, alpha=0.05):\n",
        "    x = series.dropna()\n",
        "    if len(x) < 3:\n",
        "        return np.inf\n",
        "    n = len(x)\n",
        "    t_crit = t.ppf(1 - alpha / (2 * n), n - 2)\n",
        "    return ((n - 1) / np.sqrt(n)) * np.sqrt(t_crit**2 / (n - 2 + t_crit**2))\n",
        "\n",
        "gcrit = train[numeric_for_outliers].apply(grubbs_threshold)\n",
        "gstat_train = np.abs(train[numeric_for_outliers] - train_mean) / train_std\n",
        "gstat_test = np.abs(test[numeric_for_outliers] - train_mean) / train_std\n",
        "grubbs_mask_train = gstat_train.gt(gcrit)\n",
        "grubbs_mask_test = gstat_test.gt(gcrit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pFe3NKD_4M1"
      },
      "outputs": [],
      "source": [
        "# Сводка по выбросам\n",
        "stat_outlier_mask_train = z_mask_train | iqr_mask_train | grubbs_mask_train\n",
        "stat_outlier_mask_test = z_mask_test | iqr_mask_test | grubbs_mask_test\n",
        "\n",
        "for df, mask, name in [\n",
        "    (train, stat_outlier_mask_train, \"train\"),\n",
        "    (test, stat_outlier_mask_test, \"test\"),\n",
        "]:\n",
        "    df[\"stat_outlier_any\"] = mask.any(axis=1).astype(int)\n",
        "    df[\"stat_outlier_count\"] = mask.sum(axis=1)\n",
        "    for col in numeric_for_outliers:\n",
        "        df[f\"{col}_outlier\"] = mask[col].astype(int)\n",
        "\n",
        "print(\"Исключены бинарные признаки:\", binary_cols)\n",
        "print(\"Доля строк с хотя бы одним выбросом (train):\", train[\"stat_outlier_any\"].mean().round(3))\n",
        "print(\"Топ признаков по количеству выбросов (train):\")\n",
        "display(stat_outlier_mask_train.sum().sort_values(ascending=False).head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTsQgBs__4M1"
      },
      "outputs": [],
      "source": [
        "# Визуализация выбросов (взяты со сводки)\n",
        "cols_to_plot = [\"Age\", \"NumOfProducts\", \"CreditScore\", \"Tenure\"]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "for ax, col in zip(axes.ravel(), cols_to_plot):\n",
        "    sns.boxplot(x=train[col], ax=ax, color=\"#7fb3d5\")\n",
        "    sns.stripplot(\n",
        "        x=train[col],\n",
        "        data=train[train[f\"{col}_outlier\"] == 1],\n",
        "        ax=ax,\n",
        "        color=\"red\",\n",
        "        size=3,\n",
        "        alpha=0.6,\n",
        "        label=\"Выбросы\",\n",
        "    )\n",
        "    ax.set_title(f\"{col}: выбросы по Z/IQR/Grubbs\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "save_and_display(fig, \"outliers_boxplot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p81qQU3_4M1"
      },
      "source": [
        "**Age**: выбросы с краю (58+ лет). Их немного и, в целом, бизнес‑логика допускает пожилых клиентов. Вероятность шумовых записей низкая, поэтому оставим как есть, но используем флаг Age_outlier\n",
        "\n",
        "**NumOfProducts**: выбросы на значении 4 (редкий пакет из 4 продуктов). Это край редкого сегмента, но бизнес‑возможный. Удалять не нужно: оставим и будем использовать флаг NumOfProducts_outlier как индикатор спецпредложения\n",
        "\n",
        "**CreditScore**: выбросы в районе низких (<450) и высоких (>850) значений. Это разумные экстремумы (очень плохой / очень хороший скоринг). Удалять не стоит, так что тоже добавим флаг CreditScore_outlier\n",
        "\n",
        "**Tenure**: выброс вообще всего один. Удалять не нужно, но на всякий случай отметим флагом"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAM46PW3_4M1"
      },
      "outputs": [],
      "source": [
        "# Признак плотности (kNN дистанции)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train[numeric_for_outliers])\n",
        "X_test = scaler.transform(test[numeric_for_outliers].fillna(0))\n",
        "nn = NearestNeighbors(n_neighbors=5).fit(X_train)\n",
        "train_dists, _ = nn.kneighbors(X_train)\n",
        "test_dists, _ = nn.kneighbors(X_test)\n",
        "train[\"knn_mean_dist\"] = train_dists[:, 1:].mean(axis=1)\n",
        "test[\"knn_mean_dist\"] = test_dists[:, 1:].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKi5Q5Sk_4M1"
      },
      "outputs": [],
      "source": [
        "# ML-методы выбросов (IsolationForest, LOF)\n",
        "iso = IsolationForest(contamination=0.02, random_state=42)\n",
        "train[\"iso_outlier\"] = (iso.fit_predict(X_train) == -1).astype(int)\n",
        "train[\"iso_score\"] = iso.decision_function(X_train)\n",
        "test[\"iso_outlier\"] = (iso.predict(X_test) == -1).astype(int)\n",
        "test[\"iso_score\"] = iso.decision_function(X_test)\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.02, novelty=True)\n",
        "lof.fit(X_train)\n",
        "train[\"lof_outlier\"] = (lof.predict(X_train) == -1).astype(int)\n",
        "train[\"lof_score\"] = -lof.decision_function(X_train)\n",
        "test[\"lof_outlier\"] = (lof.predict(X_test) == -1).astype(int)\n",
        "test[\"lof_score\"] = -lof.decision_function(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owVf5yT4_4M2"
      },
      "outputs": [],
      "source": [
        "# Визуализация ML-меток\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "sns.scatterplot(\n",
        "    x=train[\"CreditScore\"],\n",
        "    y=train[\"Balance\"],\n",
        "    hue=train[\"iso_outlier\"],\n",
        "    palette=[\"steelblue\", \"red\"],\n",
        "    alpha=0.6,\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Isolation Forest: CreditScore vs Balance\")\n",
        "save_and_display(fig, \"iso_cs_balance.png\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 4))\n",
        "sns.scatterplot(\n",
        "    x=train[\"Age\"],\n",
        "    y=train[\"EstimatedSalary\"],\n",
        "    hue=train[\"lof_outlier\"],\n",
        "    palette=[\"steelblue\", \"darkorange\"],\n",
        "    alpha=0.6,\n",
        "    ax=ax,\n",
        ")\n",
        "ax.set_title(\"Local Outlier Factor: Age vs EstimatedSalary\")\n",
        "save_and_display(fig, \"lof_age_salary.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPukgscd_4M2"
      },
      "outputs": [],
      "source": [
        "# Рассчитываем метрики, считая что целевые аномалии — это реальные уходы клиентов\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "y_true = train[target].astype(int)\n",
        "\n",
        "def print_metrics(name, y_pred, score):\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
        "    auc = roc_auc_score(y_true, score)\n",
        "    print(f\"{name}: precision={p:.3f}, recall={r:.3f}, f1={f1:.3f}, roc_auc={auc:.3f}\")\n",
        "\n",
        "print_metrics(\"Isolation Forest\", train[\"iso_outlier\"], -train[\"iso_score\"])  # чем ниже score, тем аномальнее\n",
        "print_metrics(\"LOF\", train[\"lof_outlier\"], -train[\"lof_score\"]) # выше lof_score = более аномально\n",
        "\n",
        "both_anom = (train[\"iso_outlier\"] == 1) & (train[\"lof_outlier\"] == 1)\n",
        "only_iso = (train[\"iso_outlier\"] == 1) & (train[\"lof_outlier\"] == 0)\n",
        "only_lof = (train[\"iso_outlier\"] == 0) & (train[\"lof_outlier\"] == 1)\n",
        "\n",
        "print(\"\\nДоля пересечения (оба метят):\", both_anom.mean().round(3))\n",
        "print(\"Доля только IsolationForest:\", only_iso.mean().round(3))\n",
        "print(\"Доля только LOF:\", only_lof.mean().round(3))\n",
        "\n",
        "print(\"\\nПересечение с таргетом среди обоих методов:\")\n",
        "display(pd.crosstab(both_anom, y_true, normalize=\"index\"))\n",
        "\n",
        "print(\"Пересечение с таргетом среди only_iso:\")\n",
        "display(pd.crosstab(only_iso, y_true, normalize=\"index\"))\n",
        "\n",
        "print(\"Пересечение с таргетом среди only_lof:\")\n",
        "display(pd.crosstab(only_lof, y_true, normalize=\"index\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMbQWfH_4M2"
      },
      "source": [
        "Isolation Forest ловит больше целевых: LOF существенно слабее. Для признаков лучше использовать метки IF, а пересечение IF и LOF можно трактовать как жёсткие аномалии с высоким churn. LOF сейчас отдельно несёт мало пользы при текущих параметрах"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Генерация признаков и отбор переменных\n"
      ],
      "metadata": {
        "id": "Orlgj6sV_6iC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Шаг 1. Обработайте категориальные переменные\n",
        "#Тarget Encoding для одной переменной\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "target = 'Exited'\n",
        "te_col = 'Geography'\n",
        "te_new_col = te_col + '_te'\n",
        "\n",
        "train[te_new_col] = np.nan\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for train_idx, val_idx in kf.split(train, train[target]):\n",
        "    fold_train = train.iloc[train_idx]\n",
        "    fold_mapping = fold_train.groupby(te_col)[target].mean()\n",
        "\n",
        "    train.loc[val_idx, te_new_col] = train.loc[val_idx, te_col].map(fold_mapping)\n",
        "\n",
        "global_mapping = train.groupby(te_col)[target].mean()\n",
        "global_mean = train[target].mean()\n",
        "\n",
        "test[te_new_col] = test[te_col].map(global_mapping)\n",
        "\n",
        "train[te_new_col].fillna(global_mean, inplace=True)\n",
        "test[te_new_col].fillna(global_mean, inplace=True)"
      ],
      "metadata": {
        "id": "Nj4t3Q5uCmJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Лодировка остальных категориальных признаков\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "categorical_cols = ['Gender']\n",
        "label_encoders = {}"
      ],
      "metadata": {
        "id": "BdmWqiHPDM5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geography - target encoding, потому что мало категорий, но связь с таргетом может быть сложной, TE даёт компактный числовой признак, хорошо работающий и с линейными моделями, и нет\n"
      ],
      "metadata": {
        "id": "PIlHWzBgDTTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Зафиксировать промежуточные метрики\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "feature_cols = [c for c in train.columns if c != target]\n",
        "\n",
        "X = train[feature_cols]\n",
        "y = train[target]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_valid)\n",
        "y_proba = log_reg.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_valid, y_proba)\n",
        "print('roc_auc:', roc_auc)\n",
        "print()\n",
        "print(classification_report(y_valid, y_pred))"
      ],
      "metadata": {
        "id": "uIorVxeBDalG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После кодировки категориальных признаков и обучения базовой логистической регрессии с учётом дисбаланса классовмодель показала:\n",
        "\n",
        "ROC-AUC ≈ 0.89\n",
        "\n",
        "accuracy ≈ 0.81\n",
        "\n",
        "recall по классу \"ушел\" ≈ 0.81, precision ≈ 0.52.\n",
        "\n",
        "Это означает, что модель хорошо различает склонных к уходу клиентов и позволяет находить около 80% реально уходящих клиентов, хотя при этом часть оставшихся помечается как потенциально уходящие. Эти значения будут использоваться как промежуточная точка сравнения при дальнейшем обогащении признаков и их отборе."
      ],
      "metadata": {
        "id": "VTMCT5cbEO8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Шаг 2. Добавьте признаки, основанные на ближайших соседях\n",
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "knn_features = numeric_for_outliers\n",
        "\n",
        "medians = train[knn_features].median()\n",
        "\n",
        "train_knn = train[knn_features].fillna(medians)\n",
        "test_knn = test[knn_features].fillna(medians)\n",
        "\n",
        "scaler_knn = StandardScaler()\n",
        "X_train_knn = scaler_knn.fit_transform(train_knn)\n",
        "X_test_knn = scaler_knn.transform(test_knn)\n",
        "\n",
        "k = 5\n",
        "nn = NearestNeighbors(n_neighbors=k)\n",
        "nn.fit(X_train_knn)\n",
        "\n",
        "train_dists, _ = nn.kneighbors(X_train_knn)\n",
        "test_dists, _ = nn.kneighbors(X_test_knn)\n",
        "\n",
        "train_dists_no_self = train_dists[:, 1:]\n",
        "\n",
        "train['knn_mean_dist'] = train_dists_no_self.mean(axis=1)\n",
        "test['knn_mean_dist'] = test_dists.mean(axis=1)\n",
        "\n",
        "train['knn_min_dist'] = train_dists_no_self.min(axis=1)\n",
        "test['knn_min_dist'] = test_dists.min(axis=1)\n",
        "\n",
        "train['knn_max_dist'] = train_dists_no_self.max(axis=1)\n",
        "test['knn_max_dist'] = test_dists.max(axis=1)\n"
      ],
      "metadata": {
        "id": "8JrQJW-UGNvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перед построением признаков на основе ближайших соседей были заполнены пропуски в числовых переменных. Для каждого признака использовалась медиана по обучающей выборке, которая затем применялась как к train, так и к test. Это позволило корректно использовать методы, не поддерживающие значения NaN (например, NearestNeighbors), и при этом не использовать информацию из тестовой выборки."
      ],
      "metadata": {
        "id": "8xOU1j-8TP4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "target = 'Exited'\n",
        "feature_cols = [c for c in train.columns if c != target]\n",
        "\n",
        "X = train[feature_cols]\n",
        "y = train[target]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_valid)\n",
        "y_proba = log_reg.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_valid, y_proba)\n",
        "print('roc_auc:', roc_auc)\n",
        "print()\n",
        "print(classification_report(y_valid, y_pred))\n"
      ],
      "metadata": {
        "id": "oW9kPRVCLtmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Шаг 3. Обработка временных признаков\n",
        "\n",
        "В исходном датасете отсутствуют явные временные признаки.\n",
        "Переменная `Tenure` отражает стаж клиента в банке в годах и уже используется как обычный числовой признак, но не является периодической временной компонентой (её нет смысла разворачивать через синус/косинус-кодирование).\n",
        "\n"
      ],
      "metadata": {
        "id": "EiHSUVGQHlmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Шаг 4. Сформируйте контекстные признаки, отражающие специфику вашей задачи\n",
        "\n",
        "train['balance_to_salary'] = train['Balance'] / (train['EstimatedSalary'] + 1)\n",
        "test['balance_to_salary'] = test['Balance'] / (test['EstimatedSalary'] + 1)\n",
        "\n",
        "high_balance_threshold = train['Balance'].quantile(0.9)\n",
        "\n",
        "train['is_high_value_client'] = (\n",
        "    (train['Balance'] > high_balance_threshold) & (train['NumOfProducts'] > 1)\n",
        ").astype(int)\n",
        "\n",
        "test['is_high_value_client'] = (\n",
        "    (test['Balance'] > high_balance_threshold) & (test['NumOfProducts'] > 1)\n",
        ").astype(int)\n",
        "\n",
        "train['is_new_client'] = (train['Tenure'] <= 1).astype(int)\n",
        "test['is_new_client'] = (test['Tenure'] <= 1).astype(int)\n",
        "\n",
        "train['is_senior'] = (train['Age'] >= 60).astype(int)\n",
        "test['is_senior'] = (test['Age'] >= 60).astype(int)"
      ],
      "metadata": {
        "id": "DlAVWXuAIaCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "balance_to_salary - это отношение баланса клиента к его оценочной зарплате.\n",
        "Мы предполагаем, что клиенты, у которых на счетах лежит аномально большой объем средств относительно их дохода, могут быть более ценными и более чувствительными к условиям банка, и это может влиять на вероятность ухода\n",
        "\n",
        "is_high_value_client - бинарный признак, равный 1 - если клиент входит в верхние 10% по балансу и одновременно пользуется более чем одним продуктом банка.\n",
        "Мы предполагаем, что премиальные клиенты ведут себя иначе, чем обычные, например, их отток особенно важен для банка, и их поведение может отличаться от остальной массы\n",
        "\n",
        "is_new_client - индикатор новых клиентов (стаж в банке <= 1 год).\n",
        "Мы предполагаем, что клиенты в начале жизненного цикла чаще уходят, поэтому ранний отток рассматриваем как отдельный сценарий\n",
        "\n",
        "is_senior - индикатор клиентов старшего возраста (Age >= 60).\n",
        "Мы предполагаем, что поведение и потребности возрастных клиентов могут отличаться от более молодых, что может отражаться на вероятности ухода\n"
      ],
      "metadata": {
        "id": "vEaiq2LvIdiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, classification_report\n",
        "\n",
        "target = 'Exited'\n",
        "feature_cols = [c for c in train.columns if c != target]\n",
        "\n",
        "X = train[feature_cols]\n",
        "y = train[target]\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg.predict(X_valid)\n",
        "y_proba = log_reg.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_valid, y_proba)\n",
        "print('roc_auc:', roc_auc)\n",
        "print()\n",
        "print(classification_report(y_valid, y_pred))\n"
      ],
      "metadata": {
        "id": "hLjbMY1vJZGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "После добавления признаков на основе kNN и контекстных бизнес-гипотез качество базовой логистической регрессии практически не изменилось: ROC-AUC вырос с ≈0.8922 до ≈0.8925, а остальные метрики остались на похожем уровне.\n",
        "\n",
        "Это значит, что исходный набор признаков уже содержал большую часть информации, важной для линейной модели. Однако новые признаки могут оказаться более полезными для нелинейных моделей, а также для интерпретации поведения разных сегментов клиентов."
      ],
      "metadata": {
        "id": "ZIdPaw4VKNKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'Exited'\n",
        "feature_cols = [c for c in train.columns if c != target]\n",
        "\n",
        "X = train[feature_cols].copy()\n",
        "y = train[target].copy()\n",
        "\n",
        "X = X.fillna(X.median())"
      ],
      "metadata": {
        "id": "FoaUbAUjJ15U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Отбор через фильтры\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "corr_with_target = X[numeric_cols].corrwith(y).abs().sort_values(ascending=False)\n",
        "corr_with_target.head(20)\n"
      ],
      "metadata": {
        "id": "B15e5n6UJ3_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#χ²\n",
        "\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler_mm = MinMaxScaler()\n",
        "X_chi2 = scaler_mm.fit_transform(X)\n",
        "\n",
        "chi2_selector = SelectKBest(score_func=chi2, k='all')\n",
        "chi2_selector.fit(X_chi2, y)\n",
        "\n",
        "chi2_scores = pd.Series(chi2_selector.scores_, index=feature_cols).sort_values(ascending=False)\n",
        "chi2_scores.head(20)\n"
      ],
      "metadata": {
        "id": "8Q4tR15XKYw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANOVA\n",
        "\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "f_selector = SelectKBest(score_func=f_classif, k='all')\n",
        "f_selector.fit(X, y)\n",
        "\n",
        "anova_scores = pd.Series(f_selector.scores_, index=feature_cols).sort_values(ascending=False)\n",
        "anova_scores.head(20)"
      ],
      "metadata": {
        "id": "zv72rSLdKd1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все три фильтра говорят одно и то же:\n",
        "\n",
        "Очень важные признаки: Age, NumOfProducts, Geography_te, IsActiveMember, iso_score и iso_outlier, knn (mean/max/min dist), бинарки аномалий (stat_outlier_any, stat_outlier_count, Age_outlier, lof_outlier), Gender, Balance\n",
        "\n",
        "Менее важные: сырой Geography, CreditScore, более слабые флаги вроде is_new_client, lof_score"
      ],
      "metadata": {
        "id": "OM8HPTXdLAob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Обертки: RFECV с логистической регрессией\n",
        "\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "base_clf = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "rfecv = RFECV(\n",
        "    estimator=base_clf,\n",
        "    step=1,\n",
        "    cv=cv,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rfecv.fit(X, y)\n",
        "\n",
        "selected_rfecv = [feature_cols[i] for i, flag in enumerate(rfecv.support_) if flag]\n",
        "\n",
        "print('Оптимальное число признаков по RFECV:', rfecv.n_features_)\n",
        "print('Признаки, отобранные RFECV:')\n",
        "selected_rfecv\n"
      ],
      "metadata": {
        "id": "1vW2_0XdKlLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#L1-регуляризация\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "l1_clf = LogisticRegression(\n",
        "    penalty='l1',\n",
        "    solver='liblinear',\n",
        "    class_weight='balanced',\n",
        "    max_iter=2000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "l1_clf.fit(X, y)\n",
        "\n",
        "coef_abs = np.abs(l1_clf.coef_[0])\n",
        "l1_importance = pd.Series(coef_abs, index=feature_cols).sort_values(ascending=False)\n",
        "\n",
        "selected_l1 = l1_importance[l1_importance > 0].index.tolist()\n",
        "\n",
        "print('Количество признаков с ненулевым весом (L1):', len(selected_l1))\n",
        "print('Топ по важности (L1):')\n",
        "l1_importance.head(20)\n"
      ],
      "metadata": {
        "id": "4aspSSqzNX-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature importances Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=20,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X, y)\n",
        "\n",
        "rf_importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
        "\n",
        "print('Топ-20 признаков по важности:')\n",
        "rf_importances.head(20)\n"
      ],
      "metadata": {
        "id": "wdW_SisbNboc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Сбор финального списка признаков\n",
        "top_rf_features = rf_importances.head(20).index.tolist()\n",
        "\n",
        "selected_features = sorted(set(selected_rfecv) | set(selected_l1) | set(top_rf_features))\n",
        "\n",
        "print('Итого признаков:', len(selected_features))\n",
        "selected_features"
      ],
      "metadata": {
        "id": "kRa5jJ9WNeIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Нестабильные во времени признаки\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "importances_per_fold = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(X, y):\n",
        "    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
        "\n",
        "    rf_fold = RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        max_depth=7,\n",
        "        min_samples_leaf=20,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf_fold.fit(X_tr, y_tr)\n",
        "    importances_per_fold.append(rf_fold.feature_importances_)\n",
        "\n",
        "importances_per_fold = np.vstack(importances_per_fold)\n",
        "\n",
        "importances_mean = importances_per_fold.mean(axis=0)\n",
        "importances_std = importances_per_fold.std(axis=0)\n",
        "\n",
        "instability = importances_std / (importances_mean + 1e-6)\n",
        "instability_series = pd.Series(instability, index=feature_cols).sort_values(ascending=False)\n",
        "\n",
        "print('Топ-20 наиболее нестабильных признаков):')\n",
        "instability_series.head(20)"
      ],
      "metadata": {
        "id": "24P6bu9UNo58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = [\n",
        "    'Geography',\n",
        "    'Gender',\n",
        "    'Age',\n",
        "    'Tenure',\n",
        "    'NumOfProducts',\n",
        "    'HasCrCard',\n",
        "    'IsActiveMember',\n",
        "    'stat_outlier_any',\n",
        "    'stat_outlier_count',\n",
        "    'CreditScore_outlier',\n",
        "    'Age_outlier',\n",
        "    'Tenure_outlier',\n",
        "    'NumOfProducts_outlier',\n",
        "    'knn_mean_dist',\n",
        "    'iso_outlier',\n",
        "    'iso_score',\n",
        "    'lof_outlier',\n",
        "    'lof_score',\n",
        "    'Geography_te',\n",
        "    'knn_min_dist',\n",
        "    'knn_max_dist',\n",
        "    'balance_to_salary',\n",
        "    'is_high_value_client',\n",
        "    'is_new_client',\n",
        "    'is_senior'\n",
        "]"
      ],
      "metadata": {
        "id": "NihObHnUO56H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Фильтровые подходы показали, что сильнее всего с таргетом связаны базовые признаки (`Age`, `NumOfProducts`, `IsActiveMember`, `Balance`, `Gender`) и сгенерированные фичи. При этом исходные `Geography` и `CreditScore` оказались менее информативными\n",
        "\n",
        "RFECV на логистической регрессии отобрал 25 признаков, включающих основные параметры клиента, географию после TE, признаки аномальности, kNN-фичи и контекстные переменные вроде `balance_to_salary` и индикаторов разных групп клиентов.\n",
        "\n",
        "Встроенные методы (L1 и RandomForest) в целом подтвердили важность тех же признаков. Некоторые редкие бинарные признаки оказались менее стабильными.\n",
        "\n",
        "Анализ нестабильности через разные фолды показал, что сильнее всего меняются редкие флаги и аномальные признаки, тогда как ключевые фичи остаются устойчивыми.\n",
        "\n",
        "В итоге для дальнейшего моделирования выбран набор признаков, который подтверждён всеми тремя подходами и хорошо отражает как основные характеристики клиента, так и важные гипотезы о его поведении.\n"
      ],
      "metadata": {
        "id": "dahO8NI7RAEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Интерпретация и диагностика моделей"
      ],
      "metadata": {
        "id": "aiE653ZqUaZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 1. Проинтерпретируйте модели"
      ],
      "metadata": {
        "id": "jX7M0GaWP7q3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Подготовка данных для интерпретаций SHAP и LIME\n",
        "target = 'Exited'\n",
        "feature_cols = [c for c in train.columns if c != target]\n",
        "\n",
        "X_full = train[feature_cols].copy()\n",
        "y_full = train[target].copy()\n",
        "\n",
        "X_full = X_full.fillna(X_full.median(numeric_only=True))\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_full,\n",
        "    y_full,\n",
        "    test_size=0.2,\n",
        "    stratify=y_full,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape"
      ],
      "metadata": {
        "id": "a7DPMeOhUbES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#обучаем модели на финальном наборе признаков,\n",
        "#чтобы интерпретация была огласованной\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "log_reg = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=20,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "YPefkjucQJJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shap для логистической регрессии\n",
        "import shap\n",
        "\n",
        "shap.initjs()\n",
        "explainer_log_reg = shap.LinearExplainer(\n",
        "    log_reg,\n",
        "    X_train,\n",
        "    feature_perturbation='interventional'\n",
        ")\n",
        "\n",
        "shap_values_log_reg = explainer_log_reg.shap_values(X_valid)\n",
        "\n",
        "shap.summary_plot(\n",
        "    shap_values_log_reg,\n",
        "    X_valid,\n",
        "    plot_type='bar',\n",
        "    show=True\n",
        ")\n",
        "shap.summary_plot(\n",
        "    shap_values_log_reg,\n",
        "    X_valid,\n",
        "    show=True\n",
        ")"
      ],
      "metadata": {
        "id": "CxYJJ7vRQjnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тут все довольно ожидаемо. Самые важные признаки: Age, NumOfProducts и IsActiveMember. По второму графику видно, что большой возраст увеличивает вероятность ухода, а большое число продуктов и активность клиента наоборот уменьшают вероятность ухода.\n",
        "Признаки вроде Gender, CreditScore и kNN-фичи тоже что-то дают модели, но заметно слабее. Остальные признаки почти не влияют.\n",
        "\n",
        "SHAP показывает, что модель живет довольно простой логикой"
      ],
      "metadata": {
        "id": "35gQQPD_Qmv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shap для RandomFores\n",
        "import shap\n",
        "\n",
        "explainer_rf = shap.TreeExplainer(rf)\n",
        "shap_values_rf = explainer_rf.shap_values(X_valid)\n",
        "\n",
        "if isinstance(shap_values_rf, list):\n",
        "    shap_values_rf_class1 = shap_values_rf[1]\n",
        "else:\n",
        "    if shap_values_rf.ndim == 3:\n",
        "        shap_values_rf_class1 = shap_values_rf[:, :, 1]\n",
        "    else:\n",
        "        shap_values_rf_class1 = shap_values_rf\n",
        "\n",
        "#графики\n",
        "shap.summary_plot(\n",
        "    shap_values_rf_class1,\n",
        "    X_valid,\n",
        "    plot_type='bar',\n",
        "    show=True\n",
        ")\n",
        "\n",
        "shap.summary_plot(\n",
        "    shap_values_rf_class1,\n",
        "    X_valid,\n",
        "    show=True\n",
        ")"
      ],
      "metadata": {
        "id": "1aCsb4yoQl_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomForest подтверждает то же, что и линейная модель - главное: возраст, продукты и активность"
      ],
      "metadata": {
        "id": "KAXg8kiVQs5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install lime"
      ],
      "metadata": {
        "id": "oqiLzWVmQvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#настройка LIME\n",
        "\n",
        "import lime\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "\n",
        "class_names = ['stayed', 'exited']\n",
        "lime_explainer = LimeTabularExplainer(\n",
        "    training_data=np.array(X_train),\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    class_names=class_names,\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")"
      ],
      "metadata": {
        "id": "DzSR4fAOQyKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lime для логистической регрессии\n",
        "\n",
        "n_samples = min(200, X_valid.shape[0])\n",
        "np.random.seed(42)\n",
        "\n",
        "sample_indices = np.random.choice(\n",
        "    X_valid.index,\n",
        "    size=n_samples,\n",
        "    replace=False\n",
        ")\n",
        "\n",
        "lime_importances_log = pd.Series(\n",
        "    0.0,\n",
        "    index=X_train.columns\n",
        ")\n",
        "\n",
        "for idx in sample_indices:\n",
        "    exp = lime_explainer.explain_instance(\n",
        "        data_row=X_valid.loc[idx].values,\n",
        "        predict_fn=log_reg.predict_proba,\n",
        "        num_features=10\n",
        "    )\n",
        "    for feat_idx, weight in exp.local_exp[1]:\n",
        "        feat_name = X_train.columns[feat_idx]\n",
        "        lime_importances_log[feat_name] += abs(weight)\n",
        "\n",
        "lime_importances_log = lime_importances_log / n_samples\n",
        "lime_importances_log = lime_importances_log.sort_values(ascending=False)\n",
        "\n",
        "top_n = 20\n",
        "plt.figure(figsize=(8, 6))\n",
        "lime_importances_log.head(top_n).plot(kind='barh')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('LIME: глобальная важность признаков для логистической регрессии')\n",
        "plt.xlabel('среднее |влияние| по объектам')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CKe6HvpjQ0WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Снова ничего нового. По графику видно, что модель сильнее всего опирается на NumOfProducts, а также на IsActiveMember, Age, Gender и balance_to_salary.\n",
        "Фичи, связанные с выбросами и аномалиями, появляются в списке, но их вклад скорее вторичный."
      ],
      "metadata": {
        "id": "4cs8X9-LQ55W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lime для RandomForest\n",
        "\n",
        "lime_importances_rf = pd.Series(\n",
        "    0.0,\n",
        "    index=X_train.columns\n",
        ")\n",
        "\n",
        "for idx in sample_indices:\n",
        "    exp = lime_explainer.explain_instance(\n",
        "        data_row=X_valid.loc[idx].values,\n",
        "        predict_fn=rf.predict_proba,\n",
        "        num_features=10\n",
        "    )\n",
        "    for feat_idx, weight in exp.local_exp[1]:\n",
        "        feat_name = X_train.columns[feat_idx]\n",
        "        lime_importances_rf[feat_name] += abs(weight)\n",
        "\n",
        "lime_importances_rf = lime_importances_rf / n_samples\n",
        "lime_importances_rf = lime_importances_rf.sort_values(ascending=False)\n",
        "\n",
        "top_n = 20\n",
        "plt.figure(figsize=(8, 6))\n",
        "lime_importances_rf.head(top_n).plot(kind='barh')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('LIME: глобальная важность признаков для RandomForest')\n",
        "plt.xlabel('среднее |влияние| по объектам')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SS-ZaGSaQ6sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В целом LIME для RandomForest подтверждает ту же общую логику, что и все остальное"
      ],
      "metadata": {
        "id": "_ISpwSOXRALc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Общий вывод:**\n",
        "\n",
        "По результатам SHAP и LIME видно, что ключевые признаки у линейной и ансамблевой моделей в целом совпадают. В обоих случаях в топе стабильно находятся Age, NumOfProducts и IsActiveMember\n",
        "\n",
        "Направления влияния тоже согласуются: большой возраст увеличивает вероятность ухода, большое число продуктов снижает вероятность ухода, активность клиента увеличивает вероятность ухода.\n",
        "\n",
        "Второй уровень признаков (например, Gender, частично CreditScore, некоторые kNN-фичи) тоже встречается везде, но их вклад слабее. Признаки аномалий и бинарные флаги, в целом, показывают минимальное влияние.\n"
      ],
      "metadata": {
        "id": "6O3yCJ-xRDa_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#выбор одного наблюдения\n",
        "#выберем клиента, у которого вероятность ухода высокая для лучшей интерпретации\n",
        "\n",
        "proba_rf = rf.predict_proba(X_valid)[:, 1]\n",
        "idx = X_valid.index[np.argmax(proba_rf)]\n",
        "\n",
        "x_one = X_valid.loc[idx]\n",
        "print('chosen index:', idx)\n",
        "print('rf proba exited:', rf.predict_proba(x_one.to_frame().T)[0, 1])\n",
        "print('logreg proba exited:', log_reg.predict_proba(x_one.to_frame().T)[0, 1])"
      ],
      "metadata": {
        "id": "Qe7qFoqCRP9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logreg_predict_proba_with_names(x):\n",
        "    x_df = pd.DataFrame(x, columns=X_train.columns)\n",
        "    return log_reg.predict_proba(x_df)\n",
        "\n",
        "def rf_predict_proba_with_names(x):\n",
        "    x_df = pd.DataFrame(x, columns=X_train.columns)\n",
        "    return rf.predict_proba(x_df)"
      ],
      "metadata": {
        "id": "3v3LtpsxRR74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lime локальное объяснение\n",
        "\n",
        "exp_log = lime_explainer.explain_instance(\n",
        "    data_row=x_one.values,\n",
        "    predict_fn=logreg_predict_proba_with_names,\n",
        "    num_features=10\n",
        ")\n",
        "exp_rf = lime_explainer.explain_instance(\n",
        "    data_row=x_one.values,\n",
        "    predict_fn=rf_predict_proba_with_names,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "print('LIME explanation for LogisticRegression:')\n",
        "print(exp_log.as_list())\n",
        "\n",
        "print('\\nLIME explanation for RandomForest:')\n",
        "print(exp_rf.as_list())"
      ],
      "metadata": {
        "id": "-Y1vmZ6vRTs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обе модели объясняют высокий риск ухода почти одинаково. Главные факторы не поменялись (это NumOfProducts ≤ 1, Age > 42, IsActiveMember = 0 и Gender) Эти признаки сильнее всего сдвигают предсказание в сторону класса \"ушел\".\n",
        "\n",
        "В RandomForest аналогично\n"
      ],
      "metadata": {
        "id": "8-qFo33DRWb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP LogisticRegression локальное объяснение\n",
        "\n",
        "explainer_log_reg = shap.LinearExplainer(\n",
        "    log_reg,\n",
        "    X_train,\n",
        "    feature_perturbation='interventional'\n",
        ")\n",
        "\n",
        "shap_values_log = explainer_log_reg.shap_values(x_one.to_frame().T)\n",
        "\n",
        "shap.plots.waterfall(\n",
        "    shap.Explanation(\n",
        "        values=shap_values_log[0],\n",
        "        base_values=explainer_log_reg.expected_value,\n",
        "        data=x_one.values,\n",
        "        feature_names=X_train.columns\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "yiIf5yvARbkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Главный признак: Age = 60. Также заметно повышают риск NumOfProducts = 1, IsActiveMember = 0, Gender = 0 и относительно низкий CreditScore около 598.\n",
        "\n",
        "При этом признаки, связанные с выбросами (Age_outlier, stat_outlier_any/count, is_senior)  немного тормозят модель, но не перекрывают сильное влияние других признаков.\n"
      ],
      "metadata": {
        "id": "x6h1pH6PRddH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SHAP RandomForest локальное объяснение\n",
        "\n",
        "explainer_rf = shap.TreeExplainer(rf)\n",
        "sv = explainer_rf.shap_values(x_one.to_frame().T)\n",
        "\n",
        "if isinstance(sv, list):\n",
        "    sv_class1 = sv[1][0]\n",
        "    base_val = explainer_rf.expected_value[1]\n",
        "else:\n",
        "    if sv.ndim == 3:\n",
        "        sv_class1 = sv[0, :, 1]\n",
        "        base_val = explainer_rf.expected_value[1]\n",
        "    else:\n",
        "        sv_class1 = sv[0]\n",
        "        base_val = explainer_rf.expected_value\n",
        "\n",
        "shap.plots.waterfall(\n",
        "    shap.Explanation(\n",
        "        values=sv_class1,\n",
        "        base_values=base_val,\n",
        "        data=x_one.values,\n",
        "        feature_names=X_train.columns\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "ZM7gaqkKRfym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "То же самое, только вклад распределен распределен ровнее, сильнее видно Geography и немного kNN"
      ],
      "metadata": {
        "id": "OcvQRtGbRm74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Общий вывод**\n",
        "\n",
        "Для клиента LIME и SHAP дают похожую картину для логистической регрессии и для RandomForest. В обоих методах ключевыми факторами стали большой возраст, малое число продуктов и неактивность клиента, то есть основные причины высокого риска ухода совпадают.\n"
      ],
      "metadata": {
        "id": "zk7-ksD0Rn1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Построение SHAP-эмбеддингов и анализ сдвигов"
      ],
      "metadata": {
        "id": "3WeJy1p6QJsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Получение SHAP-эмбеддингов\n",
        "def get_shap_embeddings(model, X, explainer_type='tree'):\n",
        "    if explainer_type == 'tree':\n",
        "        explainer = shap.TreeExplainer(model)\n",
        "    elif explainer_type == 'linear':\n",
        "        explainer = shap.LinearExplainer(model, X, feature_perturbation='interventional')\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported explainer type\")\n",
        "\n",
        "    shap_values = explainer.shap_values(X)\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        shap_embeddings = shap_values[1]\n",
        "    elif shap_values.ndim == 3:\n",
        "        shap_embeddings = shap_values[:, :, 1]\n",
        "    else:\n",
        "        shap_embeddings = shap_values\n",
        "\n",
        "    return shap_embeddings, explainer.expected_value\n",
        "\n",
        "shap_train_rf, base_value_rf = get_shap_embeddings(rf, X_train)\n",
        "shap_valid_rf, _ = get_shap_embeddings(rf, X_valid)\n",
        "\n",
        "shap_train_df = pd.DataFrame(shap_train_rf, columns=X_train.columns)\n",
        "shap_valid_df = pd.DataFrame(shap_valid_rf, columns=X_valid.columns)"
      ],
      "metadata": {
        "id": "UxfLcaZtkGWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Выявление сдвигов и аномалий в SHAP-пространстве\n",
        "from scipy import stats\n",
        "\n",
        "drift_results = {}\n",
        "for col in shap_train_df.columns:\n",
        "    stat, p_value = stats.ks_2samp(shap_train_df[col], shap_valid_df[col])\n",
        "    drift_results[col] = {'statistic': stat, 'p_value': p_value}\n",
        "\n",
        "drift_df = pd.DataFrame(drift_results).T\n",
        "print(\"Признаки с наибольшим сдвигом в SHAP-пространстве:\")\n",
        "print(drift_df.nlargest(10, 'statistic'))\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "top_drift_features = drift_df.nlargest(6, 'statistic').index\n",
        "\n",
        "for idx, feature in enumerate(top_drift_features):\n",
        "    ax = axes[idx // 3, idx % 3]\n",
        "    ax.hist(shap_train_df[feature], alpha=0.5, label='Train', bins=30, density=True)\n",
        "    ax.hist(shap_valid_df[feature], alpha=0.5, label='Valid', bins=30, density=True)\n",
        "    ax.set_title(f'{feature}\\nKS stat: {drift_df.loc[feature, \"statistic\"]:.3f}')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y4ObsjK-kMuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Очистка данных на основе анализа сдвигов\n",
        "threshold = 3\n",
        "outlier_mask = np.zeros(len(shap_train_df), dtype=bool)\n",
        "\n",
        "for col in shap_train_df.columns:\n",
        "    col_mean = shap_train_df[col].mean()\n",
        "    col_std = shap_train_df[col].std()\n",
        "    col_outliers = np.abs(shap_train_df[col] - col_mean) > threshold * col_std\n",
        "    outlier_mask = outlier_mask | col_outliers\n",
        "\n",
        "print(f\"Доля выбросов в SHAP-пространстве: {outlier_mask.mean():.3%}\")\n",
        "\n",
        "outlier_mask_series = pd.Series(outlier_mask, index=X_train.index).astype(bool)\n",
        "\n",
        "X_train_clean = X_train[~outlier_mask_series].copy()\n",
        "y_train_clean = y_train[~outlier_mask_series].copy()"
      ],
      "metadata": {
        "id": "JhpUxfWJkR1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Переобучение модели на очищенных данных\n",
        "rf_clean = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=20,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_clean.fit(X_train_clean, y_train_clean)"
      ],
      "metadata": {
        "id": "QYtXpfdZkV8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравнение метрик до и после очистки\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "y_pred_original = rf.predict(X_valid)\n",
        "y_pred_clean = rf.predict(X_valid)\n",
        "\n",
        "y_proba_original = rf.predict_proba(X_valid)[:, 1]\n",
        "y_proba_clean = rf.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "roc_auc_original = roc_auc_score(y_valid, y_proba_original)\n",
        "roc_auc_clean = roc_auc_score(y_valid, y_proba_clean)\n",
        "\n",
        "f1_original = f1_score(y_valid, y_pred_original)\n",
        "f1_clean = f1_score(y_valid, y_pred_clean)\n",
        "\n",
        "print(\"Сравнение метрик до и после очистки:\")\n",
        "print(f\"ROC-AUC: {roc_auc_original:.4f} → {roc_auc_clean:.4f} (Δ{roc_auc_clean - roc_auc_original:+.4f})\")\n",
        "print(f\"F1-score: {f1_original:.4f} → {f1_clean:.4f} (Δ{f1_clean - f1_original:+.4f})\")"
      ],
      "metadata": {
        "id": "czNYb8VKkZJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Кластеризация SHAP-эмбеддингов\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import umap.umap_ as umap\n",
        "\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "shap_pca = pca.fit_transform(shap_train_df)\n",
        "\n",
        "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "shap_umap = umap_reducer.fit_transform(shap_train_df)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "kmeans_labels = kmeans.fit_predict(shap_train_df)\n",
        "\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
        "dbscan_labels = dbscan.fit_predict(shap_train_df)\n",
        "\n",
        "agglo = AgglomerativeClustering(n_clusters=4)\n",
        "agglo_labels = agglo.fit_predict(shap_train_df)"
      ],
      "metadata": {
        "id": "cgOBCpNSkcv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Визуализация кластеров\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "scatter = axes[0, 0].scatter(shap_pca[:, 0], shap_pca[:, 1], c=kmeans_labels, cmap='tab10', alpha=0.6)\n",
        "axes[0, 0].set_title('KMeans кластеризация (PCA)')\n",
        "axes[0, 0].set_xlabel('PC1')\n",
        "axes[0, 0].set_ylabel('PC2')\n",
        "fig.colorbar(scatter, ax=axes[0, 0])\n",
        "\n",
        "scatter = axes[0, 1].scatter(shap_umap[:, 0], shap_umap[:, 1], c=kmeans_labels, cmap='tab10', alpha=0.6)\n",
        "axes[0, 1].set_title('KMeans кластеризация (UMAP)')\n",
        "axes[0, 1].set_xlabel('UMAP1')\n",
        "axes[0, 1].set_ylabel('UMAP2')\n",
        "fig.colorbar(scatter, ax=axes[0, 1])\n",
        "\n",
        "scatter = axes[1, 0].scatter(shap_pca[:, 0], shap_pca[:, 1], c=dbscan_labels, cmap='tab10', alpha=0.6)\n",
        "axes[1, 0].set_title('DBSCAN кластеризация (PCA)')\n",
        "axes[1, 0].set_xlabel('PC1')\n",
        "axes[1, 0].set_ylabel('PC2')\n",
        "fig.colorbar(scatter, ax=axes[1, 0])\n",
        "\n",
        "scatter = axes[1, 1].scatter(shap_pca[:, 0], shap_pca[:, 1], c=agglo_labels, cmap='tab10', alpha=0.6)\n",
        "axes[1, 1].set_title('Agglomerative кластеризация (PCA)')\n",
        "axes[1, 1].set_xlabel('PC1')\n",
        "axes[1, 1].set_ylabel('PC2')\n",
        "fig.colorbar(scatter, ax=axes[1, 1])\n",
        "\n",
        "cluster_target_dist = []\n",
        "for cluster_id in np.unique(kmeans_labels):\n",
        "    mask = kmeans_labels == cluster_id\n",
        "    churn_rate = y_train[mask].mean()\n",
        "    cluster_target_dist.append((cluster_id, churn_rate, mask.sum()))\n",
        "\n",
        "cluster_target_df = pd.DataFrame(cluster_target_dist,\n",
        "                                 columns=['Cluster', 'Churn Rate', 'Size'])\n",
        "\n",
        "axes[0, 2].barh(cluster_target_df['Cluster'].astype(str),\n",
        "                cluster_target_df['Churn Rate'])\n",
        "axes[0, 2].set_title('Churn Rate по кластерам (KMeans)')\n",
        "axes[0, 2].set_xlabel('Churn Rate')\n",
        "\n",
        "axes[1, 2].pie(cluster_target_df['Size'],\n",
        "               labels=cluster_target_df['Cluster'].astype(str),\n",
        "               autopct='%1.1f%%')\n",
        "axes[1, 2].set_title('Размеры кластеров (KMeans)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0Tt5lYKCkjMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Интерпретация кластеров\n",
        "train_with_clusters = train.copy()\n",
        "train_with_clusters['shap_cluster'] = -1\n",
        "train_with_clusters.loc[X_train.index, 'shap_cluster'] = kmeans_labels\n",
        "\n",
        "cluster_profiles = []\n",
        "for cluster_id in np.unique(kmeans_labels):\n",
        "    if cluster_id == -1:\n",
        "        continue\n",
        "\n",
        "    cluster_mask = train_with_clusters['shap_cluster'] == cluster_id\n",
        "    cluster_data = train_with_clusters[cluster_mask]\n",
        "\n",
        "    profile = {\n",
        "        'cluster': cluster_id,\n",
        "        'size': len(cluster_data),\n",
        "        'churn_rate': cluster_data[target].mean()\n",
        "    }\n",
        "\n",
        "    key_features = ['Age', 'NumOfProducts', 'IsActiveMember', 'Balance', 'CreditScore']\n",
        "    for feat in key_features:\n",
        "        profile[f'{feat}_mean'] = cluster_data[feat].mean()\n",
        "\n",
        "    cluster_profiles.append(profile)\n",
        "\n",
        "cluster_profile_df = pd.DataFrame(cluster_profiles)\n",
        "print(\"\\nПрофили кластеров:\")\n",
        "print(cluster_profile_df.to_string())"
      ],
      "metadata": {
        "id": "B6R7lrLukqeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Добавление кластера как признака и переобучение\n",
        "X_train_with_cluster = X_train.copy()\n",
        "X_valid_with_cluster = X_valid.copy()\n",
        "\n",
        "X_train_with_cluster['shap_cluster'] = kmeans_labels\n",
        "X_valid_with_cluster['shap_cluster'] = kmeans.predict(shap_valid_df)\n",
        "\n",
        "rf_with_cluster = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=20,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_with_cluster.fit(X_train_with_cluster, y_train)\n",
        "\n",
        "y_proba_with_cluster = rf_with_cluster.predict_proba(X_valid_with_cluster)[:, 1]\n",
        "roc_auc_with_cluster = roc_auc_score(y_valid, y_proba_with_cluster)\n",
        "\n",
        "print(f\"\\nROC-AUC с кластерным признаком: {roc_auc_with_cluster:.4f}\")\n",
        "print(f\"Изменение относительно исходной: {roc_auc_with_cluster - roc_auc_original:+.4f}\")\n"
      ],
      "metadata": {
        "id": "5_gsqm0OQTou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы:\n",
        "\n",
        "    1. Очистка выбросов не изменила значение ROC-AUC\n",
        "    2. Кластеризация SHAP-эмбеддингов выявила 4 кластера с разной долей оттока клиентов"
      ],
      "metadata": {
        "id": "LIlGEkJ5rdHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Валидация и применение SHAP-эмбеддингов"
      ],
      "metadata": {
        "id": "gcV7LAi-QUJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Кросс-валидация с SHAP-эмбеддингами\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "X_original = X_train\n",
        "X_shap_only = shap_train_df\n",
        "X_combined = pd.concat([X_original.reset_index(drop=True),\n",
        "                       shap_train_df.reset_index(drop=True)], axis=1)\n",
        "X_combined.columns = [f\"orig_{col}\" for col in X_original.columns] + \\\n",
        "                     [f\"shap_{col}\" for col in shap_train_df.columns]\n",
        "\n",
        "rf_cv = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=7,\n",
        "    min_samples_leaf=20,\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cv_scores_original = cross_val_score(rf_cv, X_original, y_train,\n",
        "                                     cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "cv_scores_shap = cross_val_score(rf_cv, X_shap_only, y_train,\n",
        "                                 cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "cv_scores_combined = cross_val_score(rf_cv, X_combined, y_train,\n",
        "                                     cv=5, scoring='roc_auc', n_jobs=-1)\n",
        "\n",
        "print(\"\\nКросс-валидация ROC-AUC:\")\n",
        "print(f\"Только исходные признаки: {cv_scores_original.mean():.4f} (±{cv_scores_original.std():.4f})\")\n",
        "print(f\"Только SHAP-эмбеддинги: {cv_scores_shap.mean():.4f} (±{cv_scores_shap.std():.4f})\")\n",
        "print(f\"Комбинация: {cv_scores_combined.mean():.4f} (±{cv_scores_combined.std():.4f})\")"
      ],
      "metadata": {
        "id": "CsJf0b2zaNZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Построение графа взаимосвязей признаков\n",
        "import networkx as nx\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "corr_threshold = 0.3\n",
        "G = nx.Graph()\n",
        "\n",
        "for feature in X_train.columns:\n",
        "    G.add_node(feature, type='feature')\n",
        "\n",
        "for i, feat1 in enumerate(X_train.columns):\n",
        "    for feat2 in X_train.columns[i+1:]:\n",
        "        mask = X_train[feat1].notna() & X_train[feat2].notna()\n",
        "        if mask.sum() > 10:\n",
        "            corr, _ = spearmanr(X_train.loc[mask, feat1], X_train.loc[mask, feat2])\n",
        "            if not np.isnan(corr) and abs(corr) > corr_threshold:\n",
        "                G.add_edge(feat1, feat2, weight=abs(corr), correlation=corr)"
      ],
      "metadata": {
        "id": "1jgBMpJGaY73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 12))\n",
        "pos = nx.spring_layout(G, seed=42, k=0.5, iterations=50)\n",
        "\n",
        "nx.draw_networkx_nodes(\n",
        "    G, pos,\n",
        "    node_color='lightblue',\n",
        "    node_size=800,\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "edges = G.edges()\n",
        "weights = [G[u][v]['weight'] * 5 for u, v in edges]\n",
        "nx.draw_networkx_edges(\n",
        "    G, pos,\n",
        "    width=weights,\n",
        "    alpha=0.5,\n",
        "    edge_color='gray'\n",
        ")\n",
        "\n",
        "nx.draw_networkx_labels(\n",
        "    G, pos,\n",
        "    font_size=10,\n",
        "    font_weight='bold'\n",
        ")\n",
        "\n",
        "plt.title(f'Граф взаимосвязей признаков (корреляция > {corr_threshold})', fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I2YNKdh5pIQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы:\n",
        "\n",
        "    1. Обучение на SHAP-эмбеддингах дало наилучший результат, ROC-AUC = 0.9481; на втором же месте - комбинация исходных данных и SHAP-эмбеддингов, ROC-AUC = 0.9463\n",
        "    2. Был построен граф взаимосвязей признаков\n"
      ],
      "metadata": {
        "id": "o47qLh9iexiO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "I65lY5PRF64w"
      ]
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}